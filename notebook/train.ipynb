{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Get the parent directory where the src folder is located\n",
    "parent_dir = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "\n",
    "# Add the parent directory to sys.path if it's not already there\n",
    "if parent_dir not in sys.path:\n",
    "    sys.path.insert(0, parent_dir)\n",
    "\n",
    "# Now import the functions and constants from src.Files\n",
    "from src.utils import download_images\n",
    "from src.constants import allowed_units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the training data\n",
    "train_df = pd.read_csv('/Users/pragunisanotra/Desktop/Amazon ML Challenge/notebook/dataset/train.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                          image_link  group_id  entity_name  \\\n",
      "0  https://m.media-amazon.com/images/I/61I9XdN6OF...    748919  item_weight   \n",
      "1  https://m.media-amazon.com/images/I/71gSRbyXmo...    916768  item_volume   \n",
      "2  https://m.media-amazon.com/images/I/61BZ4zrjZX...    459516  item_weight   \n",
      "3  https://m.media-amazon.com/images/I/612mrlqiI4...    459516  item_weight   \n",
      "4  https://m.media-amazon.com/images/I/617Tl40LOX...    731432  item_weight   \n",
      "\n",
      "     entity_value  \n",
      "0      500.0 gram  \n",
      "1         1.0 cup  \n",
      "2      0.709 gram  \n",
      "3      0.709 gram  \n",
      "4  1400 milligram  \n"
     ]
    }
   ],
   "source": [
    "# Display the first few rows of the dataset to understand its structure\n",
    "print(train_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Data Preprocessing\n",
    "# Convert entity_value to lowercase to standardize the format\n",
    "train_df['entity_value'] = train_df['entity_value'].str.lower()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic unit cleaning - extract numeric values and units\n",
    "import re\n",
    "\n",
    "def extract_value_and_unit(entity_value):\n",
    "    if pd.isna(entity_value):\n",
    "        return None, None\n",
    "\n",
    "    # Use regex to extract numbers and words separately\n",
    "    parts = re.findall(r'[\\d.]+|\\w+', entity_value)\n",
    "\n",
    "    if len(parts) < 2:\n",
    "        return None, None\n",
    "\n",
    "    try:\n",
    "        value = float(parts[0])  # Convert the first part to a float\n",
    "        unit = ' '.join(parts[1:])  # Join the remaining parts as the unit\n",
    "        return value, unit\n",
    "    except ValueError:\n",
    "        return None, None\n",
    "\n",
    "# Apply the function to the entity_value column\n",
    "train_df['value'], train_df['unit'] = zip(*train_df['entity_value'].apply(extract_value_and_unit))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import pandas as pd\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "# Define the function to download a single image\n",
    "def download_image(url, download_folder):\n",
    "    try:\n",
    "        response = requests.get(url, stream=True)\n",
    "        response.raise_for_status()  # Check for HTTP errors\n",
    "        \n",
    "        filename = os.path.join(download_folder, url.split(\"/\")[-1])\n",
    "        with open(filename, 'wb') as f:\n",
    "            for chunk in response.iter_content(chunk_size=8192):\n",
    "                f.write(chunk)\n",
    "        print(f\"Downloaded: {url}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error downloading {url}: {e}\")\n",
    "\n",
    "def is_valid_url(url):\n",
    "    \"\"\"Check if the URL is valid and not empty.\"\"\"\n",
    "    try:\n",
    "        parsed_url = urlparse(url)\n",
    "        return all([parsed_url.scheme, parsed_url.netloc])\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "if __name__ == \"_main_\":\n",
    "    # Load your DataFrame here (adjust the path as needed)\n",
    "    # Example: train_df = pd.read_csv('path_to_your_csv_file.csv')\n",
    "    \n",
    "    # Define the folder to save images\n",
    "    download_folder = '/Users/pragunisanotra/Desktop/Amazon ML Challenge/images'\n",
    "\n",
    "    # Create the folder if it doesn't exist\n",
    "    if not os.path.exists(download_folder):\n",
    "        os.makedirs(download_folder)\n",
    "\n",
    "    # Access the 'image_link' column\n",
    "    urls = train_df['image_link']\n",
    "\n",
    "    # Filter out invalid URLs\n",
    "    valid_urls = [url for url in urls if is_valid_url(url)]\n",
    "\n",
    "    # Download images one by one\n",
    "    for url in valid_urls:\n",
    "        download_image(url, download_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Model Training\n",
    "# Example: A simple heuristic model based on statistical methods\n",
    "# (Replace with a machine learning model as required)\n",
    "\n",
    "# Create a mapping from entity_name to the most common unit in training data\n",
    "entity_unit_mapping = train_df.groupby('entity_name')['unit'].agg(lambda x: x.value_counts().idxmax())\n",
    "\n",
    "# Save mapping to use during prediction\n",
    "entity_unit_mapping.to_csv('output/entity_unit_mapping.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation file created at /Users/pragunisanotra/Desktop/Amazon ML Challenge/notebook/dataset/validation.csv with sample data.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define the data to insert into the validation.csv file with local file paths\n",
    "data = {\n",
    "    'image_link': [\n",
    "        '/Users/pragunisanotra/Desktop/Amazon ML Challenge/notebook/train_images/image_1.jpg',\n",
    "        '/Users/pragunisanotra/Desktop/Amazon ML Challenge/notebook/train_images/image_2.jpg',\n",
    "        '/Users/pragunisanotra/Desktop/Amazon ML Challenge/notebook/train_images/image_0.jpg'\n",
    "    ],\n",
    "    'entity_value': [\n",
    "        '50 gram',\n",
    "        '100 gram',\n",
    "        '75 gram'\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Create a DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Path to the validation file\n",
    "validation_file_path = '/Users/pragunisanotra/Desktop/Amazon ML Challenge/notebook/dataset/validation.csv'\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "df.to_csv(validation_file_path, index=False)\n",
    "\n",
    "print(f\"Validation file created at {validation_file_path} with sample data.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score: 0.00\n",
      "Precision: 1.00\n",
      "Recall: 0.00\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np  # Add this import for numpy\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.utils import class_weight\n",
    "\n",
    "# Load your validation data\n",
    "validation_file_path = '/Users/pragunisanotra/Desktop/Amazon ML Challenge/notebook/dataset/validation.csv'\n",
    "df = pd.read_csv(validation_file_path)\n",
    "\n",
    "# Preprocess the data\n",
    "le = LabelEncoder()\n",
    "df['entity_value'] = le.fit_transform(df['entity_value'])\n",
    "\n",
    "# Check for missing or invalid data in 'image_link' or 'entity_value'\n",
    "df = df.dropna(subset=['image_link', 'entity_value'])  # Remove rows with missing values\n",
    "\n",
    "# Feature Engineering: Using length of image_link as a dummy feature for now\n",
    "# Note: Replace this with more meaningful features in the future\n",
    "X = df['image_link'].str.len().values.reshape(-1, 1)  # Dummy feature\n",
    "y = df['entity_value']\n",
    "\n",
    "# Handle class imbalance by computing class weights based on the encoded labels\n",
    "class_weights = class_weight.compute_class_weight('balanced', classes=np.unique(y), y=y)\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create and train a model with balanced class weights\n",
    "model = RandomForestClassifier(n_estimators=100, random_state=42, class_weight='balanced')\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Calculate metrics\n",
    "f1 = f1_score(y_test, y_pred, average='weighted', zero_division=1)\n",
    "precision = precision_score(y_test, y_pred, average='weighted', zero_division=1)\n",
    "recall = recall_score(y_test, y_pred, average='weighted', zero_division=1)\n",
    "\n",
    "print(f\"F1 Score: {f1:.2f}\")\n",
    "print(f\"Precision: {precision:.2f}\")\n",
    "print(f\"Recall: {recall:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training complete. Artifacts saved.\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "import json\n",
    "import os\n",
    "\n",
    "# Define the trained model\n",
    "model = None  # Replace with your actual trained model\n",
    "\n",
    "# Save the trained model\n",
    "joblib.dump(model, 'path_to_save_your_model.pkl')\n",
    "\n",
    "# Define and save entity-unit mapping\n",
    "entity_unit_mapping = {\n",
    "    'item_weight': 'gram',\n",
    "    'length': 'centimetre',\n",
    "    # Add other mappings as needed\n",
    "}\n",
    "\n",
    "with open('entity_unit_mapping.json', 'w') as f:\n",
    "    json.dump(entity_unit_mapping, f)\n",
    "\n",
    "print(\"Training complete. Artifacts saved.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
